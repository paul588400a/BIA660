{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to word 'price'\n",
      "[(u'service', 0.7669029831886292), (u'value', 0.6675570011138916), (u'quality', 0.6538277268409729), (u'food', 0.6511838436126709), (u'reasonable', 0.6412127614021301)]\n",
      "Top 5 words similar to word 'price' but not relevant to 'bathroom'\n",
      "[(u'paying', 0.6908645629882812), (u'request', 0.6339963674545288), (u'sit', 0.6279985904693604), (u'mind', 0.6259720921516418), (u'yes', 0.6228559017181396)]\n",
      "Similarity between 'price' and 'bathroom':\n",
      "0.523103921302\n",
      "Similarity between 'price' and 'charge':\n",
      "0.466787467466\n",
      "<gensim.models.keyedvectors.KeyedVectors object at 0x1223e2250>\n",
      "[['', 'location', 'price', 'location', 'price', 'staff', 'amenities', 'amentities', 'service', '', ''], ['', 'location', 'location', 'enviornment', 'amentities', 'service', 'amentities', '']]\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_acc improved from -inf to 0.25000, saving model to best_model\n",
      " - 1s - loss: 2.6595 - acc: 0.1250 - val_loss: 2.6048 - val_acc: 0.2500\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_acc did not improve\n",
      " - 0s - loss: 2.5972 - acc: 0.2500 - val_loss: 2.5696 - val_acc: 0.1250\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_acc did not improve\n",
      " - 0s - loss: 2.5196 - acc: 0.5000 - val_loss: 2.5351 - val_acc: 0.2500\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_acc improved from 0.25000 to 0.37500, saving model to best_model\n",
      " - 0s - loss: 2.4515 - acc: 0.7500 - val_loss: 2.5020 - val_acc: 0.3750\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_acc did not improve\n",
      " - 0s - loss: 2.3460 - acc: 1.0000 - val_loss: 2.4697 - val_acc: 0.3750\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_acc improved from 0.37500 to 0.50000, saving model to best_model\n",
      " - 0s - loss: 2.3372 - acc: 1.0000 - val_loss: 2.4384 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_acc did not improve\n",
      " - 0s - loss: 2.2466 - acc: 1.0000 - val_loss: 2.4084 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_acc did not improve\n",
      " - 0s - loss: 2.1874 - acc: 1.0000 - val_loss: 2.3798 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_acc did not improve\n",
      " - 0s - loss: 2.0900 - acc: 1.0000 - val_loss: 2.3533 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_acc did not improve\n",
      " - 0s - loss: 2.0725 - acc: 1.0000 - val_loss: 2.3278 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_acc did not improve\n",
      " - 0s - loss: 2.0319 - acc: 1.0000 - val_loss: 2.3039 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_acc did not improve\n",
      " - 0s - loss: 1.9313 - acc: 1.0000 - val_loss: 2.2814 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_acc did not improve\n",
      " - 0s - loss: 1.8732 - acc: 1.0000 - val_loss: 2.2604 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_acc did not improve\n",
      " - 0s - loss: 1.8282 - acc: 1.0000 - val_loss: 2.2409 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_acc did not improve\n",
      " - 0s - loss: 1.7353 - acc: 1.0000 - val_loss: 2.2231 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_acc did not improve\n",
      " - 0s - loss: 1.6452 - acc: 1.0000 - val_loss: 2.2070 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_acc did not improve\n",
      " - 0s - loss: 1.6050 - acc: 1.0000 - val_loss: 2.1931 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_acc did not improve\n",
      " - 0s - loss: 1.5396 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_acc did not improve\n",
      " - 0s - loss: 1.5333 - acc: 1.0000 - val_loss: 2.1705 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_acc did not improve\n",
      " - 0s - loss: 1.5114 - acc: 1.0000 - val_loss: 2.1630 - val_acc: 0.5000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "                  1.00      1.00      1.00         1\n",
      "  amenities       0.00      0.00      0.00         0\n",
      " amentities       1.00      1.00      1.00         1\n",
      "enviornment       0.00      0.00      0.00         1\n",
      "   location       1.00      1.00      1.00         1\n",
      "      price       0.00      0.00      0.00         0\n",
      "    service       1.00      1.00      1.00         1\n",
      "      staff       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.80      0.80      0.80         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import json\n",
    "import gensim\n",
    "import nltk,string\n",
    "from random import shuffle\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "########### pretrain doc2vector ############\n",
    "\n",
    "with open(\"test.json\", 'r') as f:\n",
    "    reviews=[]\n",
    "    for line in f: \n",
    "        review = json.loads(line) \n",
    "        try:\n",
    "            review[\"text\"].strip().lower().encode('ascII')\n",
    "        except:\n",
    "            ;\n",
    "        else:\n",
    "            reviews.append(review[\"text\"])\n",
    "    \n",
    "    \n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in reviews]\n",
    "            \n",
    "docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "wv_model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "wv_model.build_vocab(docs) \n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    shuffle(docs)\n",
    "    wv_model.train(docs, total_examples=len(docs), epochs=1)\n",
    "        \n",
    "print(\"Top 5 words similar to word 'price'\")\n",
    "print wv_model.wv.most_similar('price', topn=5)\n",
    "\n",
    "print(\"Top 5 words similar to word 'price' but not relevant to 'bathroom'\")\n",
    "print wv_model.wv.most_similar(positive=['price','money'], negative=['bathroom'], topn=5)\n",
    "\n",
    "print(\"Similarity between 'price' and 'bathroom':\")\n",
    "print wv_model.wv.similarity('price','bathroom') \n",
    "\n",
    "print(\"Similarity between 'price' and 'charge':\")\n",
    "print wv_model.wv.similarity('price','charge') \n",
    "\n",
    "print wv_model.wv\n",
    "\n",
    "############## CNN #################\n",
    "\n",
    "\n",
    "\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "MAX_NB_WORDS=8000\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "EMBEDDING_DIM=200\n",
    "MAX_NB_WORDS=8000\n",
    "\n",
    "EMBEDDING_DIM=100\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 20\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"data_to_be_labeled.csv\",header=None)\n",
    "data.head()\n",
    "\n",
    "labels = []\n",
    "\n",
    "for subdata in data[2][0:2]:\n",
    "    label = []\n",
    "    for dd in subdata.split(\";\"):\n",
    "        for d in dd.split(\",\"):\n",
    "            label.append(d.strip())\n",
    "    labels.append(label)\n",
    "    \n",
    "print labels   \n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(labels)\n",
    "Y.shape\n",
    "mlb.classes_\n",
    "np.sum(Y, axis=0)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data[1][0:2])\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]\n",
    "\n",
    "\n",
    "voc=tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(data[1][0:2])\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "\n",
    "NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "pred=model.predict(X_test)\n",
    "pred[0:5]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_pred=np.copy(pred)\n",
    "Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "\n",
    "Y_pred[0:10]\n",
    "Y_test[0:10]\n",
    "\n",
    "print(classification_report(Y_test, Y_pred, \\\n",
    "                            target_names=mlb.classes_))\n",
    "\n",
    "\n",
    "\n",
    "############### CNN function ##################\n",
    "\n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.01):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z=Concatenate(name='concate')(conv_blocks)\n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(192, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
