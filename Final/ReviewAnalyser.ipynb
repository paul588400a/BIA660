{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############## document information ##############\n",
      "\n",
      "total_nb_words:\n",
      "1467\n",
      "\n",
      "############## label CNN performance is as follows: ##############\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  amenities       1.00      0.19      0.32        48\n",
      "environment       0.92      0.27      0.42        41\n",
      "       food       0.90      0.94      0.92       180\n",
      "   location       1.00      0.31      0.47        26\n",
      "       null       0.87      0.88      0.88       136\n",
      "      price       1.00      0.57      0.73        40\n",
      "    service       0.89      0.75      0.81       102\n",
      "  transport       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.91      0.73      0.77       575\n",
      "\n",
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/40\n",
      "Epoch 00001: val_acc improved from -inf to 0.67000, saving model to best_sent_model\n",
      " - 3s - loss: 2.4722 - acc: 0.7160 - val_loss: 2.3674 - val_acc: 0.6700\n",
      "Epoch 2/40\n",
      "Epoch 00002: val_acc improved from 0.67000 to 0.78000, saving model to best_sent_model\n",
      " - 2s - loss: 2.2202 - acc: 0.7300 - val_loss: 2.0555 - val_acc: 0.7800\n",
      "Epoch 3/40\n",
      "Epoch 00003: val_acc improved from 0.78000 to 0.83400, saving model to best_sent_model\n",
      " - 3s - loss: 2.0316 - acc: 0.7720 - val_loss: 1.8675 - val_acc: 0.8340\n",
      "Epoch 4/40\n",
      "Epoch 00004: val_acc improved from 0.83400 to 0.85000, saving model to best_sent_model\n",
      " - 2s - loss: 1.8354 - acc: 0.7920 - val_loss: 1.6733 - val_acc: 0.8500\n",
      "Epoch 5/40\n",
      "Epoch 00005: val_acc improved from 0.85000 to 0.85800, saving model to best_sent_model\n",
      " - 2s - loss: 1.6502 - acc: 0.8320 - val_loss: 1.5168 - val_acc: 0.8580\n",
      "Epoch 6/40\n",
      "Epoch 00006: val_acc improved from 0.85800 to 0.87200, saving model to best_sent_model\n",
      " - 3s - loss: 1.5159 - acc: 0.8340 - val_loss: 1.3731 - val_acc: 0.8720\n",
      "Epoch 7/40\n",
      "Epoch 00007: val_acc improved from 0.87200 to 0.89400, saving model to best_sent_model\n",
      " - 4s - loss: 1.3613 - acc: 0.8520 - val_loss: 1.2393 - val_acc: 0.8940\n",
      "Epoch 8/40\n",
      "Epoch 00008: val_acc improved from 0.89400 to 0.90600, saving model to best_sent_model\n",
      " - 3s - loss: 1.2381 - acc: 0.8600 - val_loss: 1.1238 - val_acc: 0.9060\n",
      "Epoch 9/40\n",
      "Epoch 00009: val_acc improved from 0.90600 to 0.92800, saving model to best_sent_model\n",
      " - 3s - loss: 1.1354 - acc: 0.8860 - val_loss: 1.0147 - val_acc: 0.9280\n",
      "Epoch 10/40\n",
      "Epoch 00010: val_acc improved from 0.92800 to 0.94000, saving model to best_sent_model\n",
      " - 3s - loss: 1.0410 - acc: 0.8880 - val_loss: 0.9163 - val_acc: 0.9400\n",
      "Epoch 11/40\n",
      "Epoch 00011: val_acc improved from 0.94000 to 0.94800, saving model to best_sent_model\n",
      " - 3s - loss: 0.9326 - acc: 0.9120 - val_loss: 0.8292 - val_acc: 0.9480\n",
      "Epoch 12/40\n",
      "Epoch 00012: val_acc improved from 0.94800 to 0.96600, saving model to best_sent_model\n",
      " - 3s - loss: 0.8708 - acc: 0.9040 - val_loss: 0.7503 - val_acc: 0.9660\n",
      "Epoch 13/40\n",
      "Epoch 00013: val_acc did not improve\n",
      " - 3s - loss: 0.7781 - acc: 0.9280 - val_loss: 0.6846 - val_acc: 0.9660\n",
      "Epoch 14/40\n",
      "Epoch 00014: val_acc did not improve\n",
      " - 3s - loss: 0.7312 - acc: 0.9240 - val_loss: 0.6300 - val_acc: 0.9660\n",
      "Epoch 15/40\n",
      "Epoch 00015: val_acc improved from 0.96600 to 0.97800, saving model to best_sent_model\n",
      " - 3s - loss: 0.6695 - acc: 0.9440 - val_loss: 0.5737 - val_acc: 0.9780\n",
      "Epoch 16/40\n",
      "Epoch 00016: val_acc did not improve\n",
      " - 3s - loss: 0.6410 - acc: 0.9260 - val_loss: 0.5277 - val_acc: 0.9760\n",
      "Epoch 17/40\n",
      "Epoch 00017: val_acc did not improve\n",
      " - 3s - loss: 0.5936 - acc: 0.9380 - val_loss: 0.4938 - val_acc: 0.9740\n",
      "Epoch 18/40\n",
      "Epoch 00018: val_acc did not improve\n",
      " - 3s - loss: 0.5672 - acc: 0.9260 - val_loss: 0.4522 - val_acc: 0.9760\n",
      "Epoch 19/40\n",
      "Epoch 00019: val_acc improved from 0.97800 to 0.98200, saving model to best_sent_model\n",
      " - 3s - loss: 0.5080 - acc: 0.9480 - val_loss: 0.4263 - val_acc: 0.9820\n",
      "Epoch 20/40\n",
      "Epoch 00020: val_acc did not improve\n",
      " - 3s - loss: 0.4804 - acc: 0.9440 - val_loss: 0.3976 - val_acc: 0.9760\n",
      "Epoch 21/40\n",
      "Epoch 00021: val_acc improved from 0.98200 to 0.98400, saving model to best_sent_model\n",
      " - 2s - loss: 0.4357 - acc: 0.9640 - val_loss: 0.3641 - val_acc: 0.9840\n",
      "Epoch 22/40\n",
      "Epoch 00022: val_acc improved from 0.98400 to 0.98600, saving model to best_sent_model\n",
      " - 2s - loss: 0.3927 - acc: 0.9720 - val_loss: 0.3388 - val_acc: 0.9860\n",
      "Epoch 23/40\n",
      "Epoch 00023: val_acc did not improve\n",
      " - 2s - loss: 0.3719 - acc: 0.9700 - val_loss: 0.3133 - val_acc: 0.9860\n",
      "Epoch 24/40\n",
      "Epoch 00024: val_acc did not improve\n",
      " - 2s - loss: 0.3378 - acc: 0.9780 - val_loss: 0.2953 - val_acc: 0.9840\n",
      "Epoch 25/40\n",
      "Epoch 00025: val_acc improved from 0.98600 to 0.98800, saving model to best_sent_model\n",
      " - 2s - loss: 0.3387 - acc: 0.9640 - val_loss: 0.2741 - val_acc: 0.9880\n",
      "Epoch 26/40\n",
      "Epoch 00026: val_acc improved from 0.98800 to 0.99000, saving model to best_sent_model\n",
      " - 2s - loss: 0.3054 - acc: 0.9680 - val_loss: 0.2605 - val_acc: 0.9900\n",
      "Epoch 27/40\n",
      "Epoch 00027: val_acc did not improve\n",
      " - 3s - loss: 0.3018 - acc: 0.9700 - val_loss: 0.2511 - val_acc: 0.9840\n",
      "Epoch 28/40\n",
      "Epoch 00028: val_acc did not improve\n",
      " - 4s - loss: 0.2946 - acc: 0.9600 - val_loss: 0.2308 - val_acc: 0.9860\n",
      "Epoch 29/40\n",
      "Epoch 00029: val_acc did not improve\n",
      " - 4s - loss: 0.2572 - acc: 0.9740 - val_loss: 0.2181 - val_acc: 0.9880\n",
      "Epoch 30/40\n",
      "Epoch 00030: val_acc did not improve\n",
      " - 4s - loss: 0.2690 - acc: 0.9560 - val_loss: 0.2068 - val_acc: 0.9860\n",
      "Epoch 31/40\n",
      "Epoch 00031: val_acc did not improve\n",
      " - 3s - loss: 0.2338 - acc: 0.9760 - val_loss: 0.1986 - val_acc: 0.9880\n",
      "Epoch 32/40\n",
      "Epoch 00032: val_acc did not improve\n",
      " - 3s - loss: 0.2469 - acc: 0.9580 - val_loss: 0.1824 - val_acc: 0.9900\n",
      "Epoch 33/40\n",
      "Epoch 00033: val_acc did not improve\n",
      " - 3s - loss: 0.2180 - acc: 0.9740 - val_loss: 0.1727 - val_acc: 0.9900\n",
      "Epoch 34/40\n",
      "Epoch 00034: val_acc did not improve\n",
      " - 3s - loss: 0.1975 - acc: 0.9740 - val_loss: 0.1678 - val_acc: 0.9900\n",
      "Epoch 35/40\n",
      "Epoch 00035: val_acc did not improve\n",
      " - 3s - loss: 0.1996 - acc: 0.9780 - val_loss: 0.1548 - val_acc: 0.9900\n",
      "Epoch 36/40\n",
      "Epoch 00036: val_acc did not improve\n",
      " - 3s - loss: 0.1843 - acc: 0.9820 - val_loss: 0.1470 - val_acc: 0.9900\n",
      "Epoch 37/40\n",
      "Epoch 00037: val_acc did not improve\n",
      " - 3s - loss: 0.1851 - acc: 0.9720 - val_loss: 0.1389 - val_acc: 0.9900\n",
      "Epoch 38/40\n",
      "Epoch 00038: val_acc did not improve\n",
      " - 3s - loss: 0.1673 - acc: 0.9760 - val_loss: 0.1323 - val_acc: 0.9900\n",
      "Epoch 39/40\n",
      "Epoch 00039: val_acc did not improve\n",
      " - 3s - loss: 0.1484 - acc: 0.9880 - val_loss: 0.1251 - val_acc: 0.9900\n",
      "Epoch 40/40\n",
      "Epoch 00040: val_acc improved from 0.99000 to 0.99200, saving model to best_sent_model\n",
      " - 3s - loss: 0.1431 - acc: 0.9860 - val_loss: 0.1195 - val_acc: 0.9920\n",
      "\n",
      "############## sentiment CNN performance is as follows: ##############\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       363\n",
      "          1       1.00      0.97      0.99       137\n",
      "\n",
      "avg / total       0.99      0.99      0.99       500\n",
      "\n",
      "{'the burger is good': {'service': 0.041591111570596695, 'food': 0.31713607907295227, 'price': 0.06203855946660042, 'environment': 0.04847916215658188, 'amenities': 0.066609226167202, 'location': 0.03672020137310028, 'null': 0.33462467789649963, 'transport': 0.016501232981681824}, 'the staff is nice': {'service': 0.8104432821273804, 'food': 0.24989019334316254, 'price': 0.10218176245689392, 'environment': 0.7472624182701111, 'amenities': 0.3334963023662567, 'location': 0.14497023820877075, 'null': 0.0006993734859861434, 'transport': 0.0323978029191494}}\n",
      "{'the burger is good': 0.9342824220657349, 'the staff is nice': 0.37331295013427734}\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import json\n",
    "import gensim\n",
    "import nltk,string\n",
    "from random import shuffle\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from nltk import tokenize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import matplotlib  \n",
    "matplotlib.use('Agg') \n",
    "from matplotlib.pyplot import plot,savefig \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "BEST_SENT_MODEL_FILEPATH=\"best_sent_model\"\n",
    "MAX_NB_WORDS=1467\n",
    "MAX_DOC_LEN=200\n",
    "EMBEDDING_DIM=200\n",
    "FILTER_SIZES=[2,3,4]\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 40\n",
    "\n",
    "class ReviewAnalyser(object):\n",
    "    \n",
    "    # label's cnn model\n",
    "    label_model = None\n",
    "    # label's classification: ['amenities' 'environment' 'food' 'location' 'null' 'price' 'service' 'transport']\n",
    "    label_mlb = None\n",
    "    # labels input padding sequence\n",
    "    label_padding_sequence = None\n",
    "    # labels actual classification\n",
    "    label_act = None\n",
    "    # sentiment's cnn model\n",
    "    sent_model = None\n",
    "    # sentiment's classification: ['0', '1'] 0: neutral, 1: positive/negative\n",
    "    sent_mlb = None\n",
    "    # sentiment input padding sequence\n",
    "    sent_padding_sequence = None\n",
    "    # sentiment actual classification\n",
    "    sent_act = None\n",
    "    # doc2vector's cnn model\n",
    "    wv_model = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, data): \n",
    "        self.data = data;\n",
    "        \n",
    "    @staticmethod    \n",
    "    def cnn_model(FILTER_SIZES, \\\n",
    "        # filter sizes as a list\n",
    "        MAX_NB_WORDS, \\\n",
    "        # total number of words\n",
    "        MAX_DOC_LEN, \\\n",
    "        # max words in a doc\n",
    "        NUM_OUTPUT_UNITS=1, \\\n",
    "        # number of output units\n",
    "        EMBEDDING_DIM=200, \\\n",
    "        # word vector dimension\n",
    "        NUM_FILTERS=64, \\\n",
    "        # number of filters for all size\n",
    "        DROP_OUT=0.5, \\\n",
    "        # dropout rate\n",
    "        PRETRAINED_WORD_VECTOR=None,\\\n",
    "        # Whether to use pretrained word vectors\n",
    "        LAM=0.01,\\\n",
    "        ACTIVATION='sigmoid'):            \n",
    "        # regularization coefficient\n",
    "    \n",
    "        main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                           dtype='int32', name='main_input')\n",
    "\n",
    "        if PRETRAINED_WORD_VECTOR is not None:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                            trainable=False,\\\n",
    "                            name='embedding')(main_input)\n",
    "        else:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            name='embedding')(main_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        for f in FILTER_SIZES:\n",
    "            conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                          activation='relu', name='conv_'+str(f))(embed_1)\n",
    "            conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "            conv = Flatten(name='flat_'+str(f))(conv)\n",
    "            conv_blocks.append(conv)\n",
    "\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "        drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "        dense = Dense(192, activation='relu',\\\n",
    "                        kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "        preds = Dense(NUM_OUTPUT_UNITS, activation=ACTIVATION, name='output')(dense)\n",
    "        model = Model(inputs=main_input, outputs=preds)\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", \\\n",
    "                  optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "\n",
    "        return model\n",
    "\n",
    "    # training to change document into vector using gensim\n",
    "    def pretrain(self):\n",
    "        with open(\"test.json\", 'r') as f:\n",
    "            reviews=[]\n",
    "            for line in f: \n",
    "                review = json.loads(line) \n",
    "                try:\n",
    "                    review[\"text\"].strip().lower().encode('ascII')\n",
    "                except:\n",
    "                    # do nothing\n",
    "                    a = 1\n",
    "                else:\n",
    "                    reviews.append(review[\"text\"])\n",
    "\n",
    "        sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "                     for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                         if token not in string.punctuation and \\\n",
    "                         len(token.strip(string.punctuation).strip())>=2]\\\n",
    "                     for doc in reviews]\n",
    "\n",
    "\n",
    "        docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "        self.wv_model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "        self.wv_model.build_vocab(docs) \n",
    "\n",
    "        for epoch in range(30):\n",
    "            # shuffle the documents in each epoch\n",
    "            shuffle(docs)\n",
    "            # in each epoch, all samples are used\n",
    "            self.wv_model.train(docs, total_examples=len(docs), epochs=1)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price'\")\n",
    "#         print self.wv_model.wv.most_similar('price', topn=5)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price' but not relevant to 'bathroom'\")\n",
    "#         print self.wv_model.wv.most_similar(positive=['price','money'], negative=['bathroom'], topn=5)\n",
    "\n",
    "#         print(\"Similarity between 'price' and 'bathroom':\")\n",
    "#         print self.wv_model.wv.similarity('price','bathroom') \n",
    "\n",
    "#         print(\"Similarity between 'price' and 'charge':\")\n",
    "#         print self.wv_model.wv.similarity('price','charge') \n",
    "\n",
    "#         print self.wv_model.wv\n",
    "\n",
    "    # training labels CNN\n",
    "    def trainLebels(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        # fetch labels for each sentence        \n",
    "        for subdata in self.data[2][0:500]:\n",
    "            label = []\n",
    "            for d in subdata.split(\",\"):\n",
    "                label.append(d.strip())\n",
    "            labels.append(label)\n",
    "\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        Y=mlb.fit_transform(labels)\n",
    "        self.label_act = Y\n",
    "        self.label_mlb = mlb\n",
    "        np.sum(Y, axis=0)\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.label_padding_sequence = padded_sequences\n",
    "        \n",
    "\n",
    "        NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                        padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "        self.label_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                        MAX_DOC_LEN, NUM_OUTPUT_UNITS, \\\n",
    "                        PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "        \n",
    "        if(RETRAIN == 0):\n",
    "            if os.path.exists(\"best_model\"):\n",
    "                self.label_model.load_weights(\"best_model\")\n",
    "                pred=self.label_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "        training=self.label_model.fit(X_train, Y_train, \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[X_test, Y_test], verbose=2)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    # training sentiment CNN        \n",
    "    def trainSentiment(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        for i,subdata in enumerate(self.data[3][0:500]):\n",
    "            if subdata == 1:\n",
    "                labels.append(['1'])\n",
    "            else:\n",
    "                labels.append(['0'])\n",
    "\n",
    "        Y_labels = np.copy(labels)\n",
    "        mlb = LabelBinarizer()\n",
    "        Y = mlb.fit_transform(Y_labels)\n",
    "        self.sent_act = Y\n",
    "        self.sent_mlb = mlb\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.sent_padding_sequence = padded_sequences\n",
    "\n",
    "        NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "        self.sent_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                    MAX_DOC_LEN, \\\n",
    "                    PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "        \n",
    "        if(RETRAIN == 0):\n",
    "            if os.path.exists(\"best_sent_model\"):\n",
    "                self.sent_model.load_weights(\"best_sent_model\")\n",
    "                pred=self.sent_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_SENT_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "        training=self.sent_model.fit(padded_sequences[0:500], self.data[3][0:500], \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[padded_sequences[0:500], self.data[3][0:500]], verbose=2) \n",
    "        \n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def checkPerform(model, mlb, data_tobe_predicted, Y_actual):\n",
    "        pred=model.predict(data_tobe_predicted)\n",
    "        Y_pred=np.copy(pred)\n",
    "        Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "        print(classification_report(Y_actual, Y_pred, \\\n",
    "                                    target_names=mlb.classes_))\n",
    "        return classification_report(Y_actual, Y_pred, \\\n",
    "                                    target_names=mlb.classes_)\n",
    "       \n",
    "    # check document information to determine the value of hyper-parameter\n",
    "    def checkDocInform(self):  \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        total_nb_words=len(tokenizer.word_counts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        print \"\\n############## document information ##############\\n\"\n",
    "        print \"total_nb_words:\"\n",
    "        print(total_nb_words)\n",
    "\n",
    "        word_counts=pd.DataFrame(\\\n",
    "                    tokenizer.word_counts.items(), \\\n",
    "                    columns=['word','count'])\n",
    "        df=word_counts['count'].value_counts().reset_index()\n",
    "        df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "        plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "        plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "        plt.xlabel('Word Frequency')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('1.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "        df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "        df.columns=['index','counts']\n",
    "\n",
    "        df=df.sort_values(by='index')\n",
    "        df['percent']=df['counts']/len(sen_len)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "        \n",
    "        plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "        plt.xlabel('Sentence Length')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('2.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "    # predict labels for text, need to execute trainLabels first\n",
    "    def predictLabels(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.label_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            dict1 = {}\n",
    "            pred_list = sub_pred[i].tolist()\n",
    "            for i, sub_pred_list in enumerate(pred_list):\n",
    "                dict1[self.label_mlb.classes_[i]] = pred_list[i]\n",
    "            rtn[key] = dict1\n",
    "        return rtn\n",
    "        \n",
    "    def predictSentiment(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.sent_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            rtn[key] = sub_pred[i].tolist()[0]\n",
    "        return rtn\n",
    "        \n",
    "\n",
    "data=pd.read_csv(\"data_sample.csv\",header=None)\n",
    "ra = ReviewAnalyser(data)\n",
    "ra.pretrain()\n",
    "ra.checkDocInform()\n",
    "ra.trainLebels(RETRAIN=1)\n",
    "print \"\\n############## label CNN performance is as follows: ##############\\n\"\n",
    "ReviewAnalyser.checkPerform(ra.label_model, ra.label_mlb, ra.label_padding_sequence, ra.label_act)\n",
    "ra.trainSentiment(RETRAIN=1)\n",
    "print \"\\n############## sentiment CNN performance is as follows: ##############\\n\"\n",
    "ReviewAnalyser.checkPerform(ra.sent_model, ra.sent_mlb, ra.sent_padding_sequence, ra.sent_act)\n",
    "label_predict = ra.predictLabels(text_arr=[\"the burger is good\", \"the staff is nice\"])\n",
    "print label_predict\n",
    "sentiment_predict = ra.predictSentiment(text_arr=[\"the burger is good\", \"the staff is nice\"])\n",
    "print sentiment_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
