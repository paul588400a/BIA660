{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############## label CNN performance is as follows: ##############\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  amenities       1.00      0.19      0.32        48\n",
      "environment       0.92      0.27      0.42        41\n",
      "       food       0.90      0.94      0.92       180\n",
      "   location       1.00      0.31      0.47        26\n",
      "       null       0.87      0.88      0.88       136\n",
      "      price       1.00      0.57      0.73        40\n",
      "    service       0.89      0.75      0.81       102\n",
      "  transport       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.91      0.73      0.77       575\n",
      "\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import json\n",
    "import gensim\n",
    "import nltk,string\n",
    "from random import shuffle\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from nltk import tokenize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import matplotlib  \n",
    "matplotlib.use('Agg') \n",
    "from matplotlib.pyplot import plot,savefig \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BEST_MODEL_FILEPATH=\"best_model\"\n",
    "BEST_SENT_MODEL_FILEPATH=\"best_sent_model\"\n",
    "MAX_NB_WORDS=1467\n",
    "MAX_DOC_LEN=200\n",
    "EMBEDDING_DIM=200\n",
    "FILTER_SIZES=[2,3,4]\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 40\n",
    "\n",
    "class ReviewAnalyser(object):\n",
    "    \n",
    "    # label's cnn model\n",
    "    label_model = None\n",
    "    # label's classification: ['amenities' 'environment' 'food' 'location' 'null' 'price' 'service' 'transport']\n",
    "    label_mlb = None\n",
    "    # labels input padding sequence\n",
    "    label_padding_sequence = None\n",
    "    # labels actual classification\n",
    "    label_act = None\n",
    "    # sentiment's cnn model\n",
    "    sent_model = None\n",
    "    # sentiment's classification: ['0', '1'] 0: neutral, 1: positive/negative\n",
    "    sent_mlb = None\n",
    "    # sentiment input padding sequence\n",
    "    sent_padding_sequence = None\n",
    "    # sentiment actual classification\n",
    "    sent_act = None\n",
    "    # doc2vector's cnn model\n",
    "    wv_model = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, data): \n",
    "        self.data = data;\n",
    "        \n",
    "    @staticmethod    \n",
    "    def cnn_model(FILTER_SIZES, \\\n",
    "        # filter sizes as a list\n",
    "        MAX_NB_WORDS, \\\n",
    "        # total number of words\n",
    "        MAX_DOC_LEN, \\\n",
    "        # max words in a doc\n",
    "        NUM_OUTPUT_UNITS=1, \\\n",
    "        # number of output units\n",
    "        EMBEDDING_DIM=200, \\\n",
    "        # word vector dimension\n",
    "        NUM_FILTERS=64, \\\n",
    "        # number of filters for all size\n",
    "        DROP_OUT=0.5, \\\n",
    "        # dropout rate\n",
    "        PRETRAINED_WORD_VECTOR=None,\\\n",
    "        # Whether to use pretrained word vectors\n",
    "        LAM=0.01,\\\n",
    "        ACTIVATION='sigmoid'):            \n",
    "        # regularization coefficient\n",
    "    \n",
    "        main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                           dtype='int32', name='main_input')\n",
    "\n",
    "        if PRETRAINED_WORD_VECTOR is not None:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                            trainable=False,\\\n",
    "                            name='embedding')(main_input)\n",
    "        else:\n",
    "            embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                            output_dim=EMBEDDING_DIM, \\\n",
    "                            input_length=MAX_DOC_LEN, \\\n",
    "                            name='embedding')(main_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        for f in FILTER_SIZES:\n",
    "            conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                          activation='relu', name='conv_'+str(f))(embed_1)\n",
    "            conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "            conv = Flatten(name='flat_'+str(f))(conv)\n",
    "            conv_blocks.append(conv)\n",
    "\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "        drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "        dense = Dense(192, activation='relu',\\\n",
    "                        kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "        preds = Dense(NUM_OUTPUT_UNITS, activation=ACTIVATION, name='output')(dense)\n",
    "        model = Model(inputs=main_input, outputs=preds)\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\", \\\n",
    "                  optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "\n",
    "        return model\n",
    "\n",
    "    # training to change document into vector using gensim\n",
    "    def pretrain(self):\n",
    "        with open(\"test.json\", 'r') as f:\n",
    "            reviews=[]\n",
    "            for line in f: \n",
    "                review = json.loads(line) \n",
    "                try:\n",
    "                    review[\"text\"].strip().lower().encode('ascII')\n",
    "                except:\n",
    "                    # do nothing\n",
    "                    a = 1\n",
    "                else:\n",
    "                    reviews.append(review[\"text\"])\n",
    "\n",
    "        sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "                     for token in nltk.word_tokenize(doc.lower()) \\\n",
    "                         if token not in string.punctuation and \\\n",
    "                         len(token.strip(string.punctuation).strip())>=2]\\\n",
    "                     for doc in reviews]\n",
    "\n",
    "\n",
    "        docs=[TaggedDocument(sentences[i], [str(i)]) for i in range(len(sentences)) ]\n",
    "        self.wv_model = doc2vec.Doc2Vec(dm=1, min_count=5, window=5, size=200, workers=4)\n",
    "        self.wv_model.build_vocab(docs) \n",
    "\n",
    "        for epoch in range(30):\n",
    "            # shuffle the documents in each epoch\n",
    "            shuffle(docs)\n",
    "            # in each epoch, all samples are used\n",
    "            self.wv_model.train(docs, total_examples=len(docs), epochs=1)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price'\")\n",
    "#         print self.wv_model.wv.most_similar('price', topn=5)\n",
    "\n",
    "#         print(\"Top 5 words similar to word 'price' but not relevant to 'bathroom'\")\n",
    "#         print self.wv_model.wv.most_similar(positive=['price','money'], negative=['bathroom'], topn=5)\n",
    "\n",
    "#         print(\"Similarity between 'price' and 'bathroom':\")\n",
    "#         print self.wv_model.wv.similarity('price','bathroom') \n",
    "\n",
    "#         print(\"Similarity between 'price' and 'charge':\")\n",
    "#         print self.wv_model.wv.similarity('price','charge') \n",
    "\n",
    "#         print self.wv_model.wv\n",
    "\n",
    "    # training labels CNN\n",
    "    def trainLebels(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        # fetch labels for each sentence        \n",
    "        for subdata in self.data[2][0:500]:\n",
    "            label = []\n",
    "            for d in subdata.split(\",\"):\n",
    "                label.append(d.strip())\n",
    "            labels.append(label)\n",
    "\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        Y=mlb.fit_transform(labels)\n",
    "        self.label_act = Y\n",
    "        self.label_mlb = mlb\n",
    "        np.sum(Y, axis=0)\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.label_padding_sequence = padded_sequences\n",
    "        \n",
    "\n",
    "        NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                        padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "        self.label_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                        MAX_DOC_LEN, NUM_OUTPUT_UNITS, \\\n",
    "                        PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "        \n",
    "        if(RETRAIN == 0):\n",
    "            if os.path.exists(\"best_model\"):\n",
    "                self.label_model.load_weights(\"best_model\")\n",
    "                pred=self.label_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "        training=self.label_model.fit(X_train, Y_train, \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[X_test, Y_test], verbose=2)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    # training sentiment CNN        \n",
    "    def trainSentiment(self, RETRAIN=0):\n",
    "        labels = []\n",
    "        for i,subdata in enumerate(self.data[3][0:500]):\n",
    "            if subdata == 1:\n",
    "                labels.append(['1'])\n",
    "            else:\n",
    "                labels.append(['0'])\n",
    "\n",
    "        Y_labels = np.copy(labels)\n",
    "        mlb = LabelBinarizer()\n",
    "        Y = mlb.fit_transform(Y_labels)\n",
    "        self.sent_act = Y\n",
    "        self.sent_mlb = mlb\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "        embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if i >= NUM_WORDS:\n",
    "                continue\n",
    "            if word in self.wv_model.wv:\n",
    "                embedding_matrix[i]=self.wv_model.wv[word]\n",
    "\n",
    "        voc=tokenizer.word_index\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                         maxlen=MAX_DOC_LEN, \\\n",
    "                                         padding='post', truncating='post')\n",
    "        self.sent_padding_sequence = padded_sequences\n",
    "\n",
    "        NUM_OUTPUT_UNITS=len(mlb.classes_)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "        self.sent_model=ReviewAnalyser.cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                    MAX_DOC_LEN, \\\n",
    "                    PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "        \n",
    "        if(RETRAIN == 0):\n",
    "            if os.path.exists(\"best_sent_model\"):\n",
    "                self.sent_model.load_weights(\"best_sent_model\")\n",
    "                pred=self.sent_model.predict(padded_sequences[0:500])\n",
    "                return\n",
    "\n",
    "\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "        checkpoint = ModelCheckpoint(BEST_SENT_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                                     verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "        training=self.sent_model.fit(padded_sequences[0:500], self.data[3][0:500], \\\n",
    "                  batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "                  callbacks=[earlyStopping, checkpoint],\\\n",
    "                  validation_data=[padded_sequences[0:500], self.data[3][0:500]], verbose=2) \n",
    "        \n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def checkPerform(model, mlb, data_tobe_predicted, Y_actual):\n",
    "        pred=model.predict(data_tobe_predicted)\n",
    "        Y_pred=np.copy(pred)\n",
    "        Y_pred=np.where(Y_pred>0.5,1,0)\n",
    "        print(classification_report(Y_actual, Y_pred, \\\n",
    "                                    target_names=mlb.classes_))\n",
    "        return classification_report(Y_actual, Y_pred, \\\n",
    "                                    target_names=mlb.classes_)\n",
    "       \n",
    "    # check document information to determine the value of hyper-parameter\n",
    "    def checkDocInform(self):  \n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        total_nb_words=len(tokenizer.word_counts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.data[1][0:500])\n",
    "        print \"\\n############## document information ##############\\n\"\n",
    "        print \"total_nb_words:\"\n",
    "        print(total_nb_words)\n",
    "\n",
    "        word_counts=pd.DataFrame(\\\n",
    "                    tokenizer.word_counts.items(), \\\n",
    "                    columns=['word','count'])\n",
    "        df=word_counts['count'].value_counts().reset_index()\n",
    "        df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "\n",
    "        plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "        plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "        plt.xlabel('Word Frequency')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('1.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        sen_len=pd.Series([len(item) for item in sequences])\n",
    "\n",
    "        df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "        df.columns=['index','counts']\n",
    "\n",
    "        df=df.sort_values(by='index')\n",
    "        df['percent']=df['counts']/len(sen_len)\n",
    "        df['cumsum']=df['percent'].cumsum()\n",
    "        \n",
    "        plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "        plt.xlabel('Sentence Length')\n",
    "        plt.ylabel('Percentage')\n",
    "        savefig('2.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "    # predict labels for text, need to execute trainLabels first\n",
    "    def predictLabels(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.label_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            dict1 = {}\n",
    "            pred_list = sub_pred[i].tolist()\n",
    "            for i, sub_pred_list in enumerate(pred_list):\n",
    "                dict1[self.label_mlb.classes_[i]] = pred_list[i]\n",
    "            rtn[key] = dict1\n",
    "        return rtn\n",
    "        \n",
    "    def predictSentiment(self, text_arr=[]):\n",
    "        if len(text_arr)==0:\n",
    "            return\n",
    "        rtn = {}\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(self.data[1][0:500])\n",
    "        sub_sequences = tokenizer.texts_to_sequences(text_arr)\n",
    "        padded_sub_sequences = pad_sequences(sub_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', truncating='post')\n",
    "        sub_pred = self.sent_model.predict(padded_sub_sequences)\n",
    "        for i, key in enumerate(text_arr):\n",
    "            rtn[key] = sub_pred[i].tolist()[0]\n",
    "        return rtn\n",
    "        \n",
    "data=pd.read_csv(\"data_sample.csv\",header=None)\n",
    "ra = ReviewAnalyser(data)\n",
    "ra.pretrain()\n",
    "#ra.checkDocInform()\n",
    "ra.trainLebels(RETRAIN=0)\n",
    "print \"\\n############## label CNN performance is as follows: ##############\\n\"\n",
    "print type(ReviewAnalyser.checkPerform(ra.label_model, ra.label_mlb, ra.label_padding_sequence, ra.label_act))\n",
    "#ra.trainSentiment(RETRAIN=0)\n",
    "# print \"\\n############## sentiment CNN performance is as follows: ##############\\n\"\n",
    "# ReviewAnalyser.checkPerform(ra.sent_model, ra.sent_mlb, ra.sent_padding_sequence, ra.sent_act)\n",
    "# label_predict = ra.predictLabels(text_arr=[\"the burger is good\", \"the staff is nice\"])\n",
    "# print label_predict\n",
    "# sentiment_predict = ra.predictSentiment(text_arr=[\"the burger is good\", \"the staff is nice\"])\n",
    "# print sentiment_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
