{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 9]\n",
      " [3 9 8]\n",
      " [8 3 2]\n",
      " [1 1 7]]\n",
      "[7 3 2 1]\n",
      "[2 6 6 6]\n",
      "------ task 1 ------\n",
      "input:\n",
      "[[7 7 9]\n",
      " [3 9 8]\n",
      " [8 3 2]\n",
      " [1 1 7]]\n",
      "shape of the input:\n",
      "(4, 3)\n",
      "shape of the output:\n",
      "(4, 3)\n",
      "ouput:\n",
      "[[ 0.          0.          1.        ]\n",
      " [ 0.          1.          0.83333333]\n",
      " [ 1.          0.16666667  0.        ]\n",
      " [ 0.          0.          1.        ]]\n",
      "------ end of task 1 ------\n",
      "\n",
      "------ task 2 ------\n",
      "Cluster 0: edu; mac; apple; drive; se; monitor; problem; thanks; university; quadra; does; scsi; posting; com; know; simms; nntp; use; card; host \n",
      "Cluster 1: edu; baseball; year; cs; team; game; article; writes; university; players; com; games; runs; jewish; posting; host; nntp; win; think; season \n",
      "Cluster 2: car; com; edu; cars; article; writes; hp; oil; just; engine; don; like; good; posting; usa; nntp; host; university; distribution; new \n",
      "Cluster 3: space; nasa; edu; access; henry; gov; alaska; moon; com; digex; pat; toronto; orbit; writes; article; launch; just; earth; like; lunar \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.93      0.94       578\n",
      "          1       0.89      0.94      0.91       594\n",
      "          2       0.86      0.95      0.90       597\n",
      "          3       0.98      0.86      0.91       593\n",
      "\n",
      "avg / total       0.92      0.92      0.92      2362\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predict  actual\n",
       "0        0         539\n",
       "         1           8\n",
       "         2           4\n",
       "         3          12\n",
       "1        0          15\n",
       "         1         556\n",
       "         2          24\n",
       "         3          27\n",
       "2        0          21\n",
       "         1          26\n",
       "         2         565\n",
       "         3          45\n",
       "3        0           3\n",
       "         1           4\n",
       "         2           4\n",
       "         3         509\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predict\n",
      "0          1        1\n",
      "1          1        1\n",
      "2          1        1\n",
      "3          0        0\n",
      "4          0        0\n",
      "5          1        1\n",
      "6          3        3\n",
      "7          2        2\n",
      "8          0        0\n",
      "9          3        3\n",
      "10         1        3\n",
      "11         1        1\n",
      "12         0        0\n",
      "13         3        3\n",
      "14         0        0\n",
      "15         0        0\n",
      "16         0        0\n",
      "17         2        2\n",
      "18         2        2\n",
      "19         2        2\n",
      "20         3        3\n",
      "21         0        0\n",
      "22         2        2\n",
      "23         2        2\n",
      "24         0        0\n",
      "25         0        2\n",
      "26         3        3\n",
      "27         3        3\n",
      "28         3        3\n",
      "29         3        3\n",
      "...      ...      ...\n",
      "2332       2        2\n",
      "2333       0        0\n",
      "2334       2        2\n",
      "2335       3        3\n",
      "2336       0        0\n",
      "2337       1        1\n",
      "2338       1        1\n",
      "2339       2        2\n",
      "2340       0        0\n",
      "2341       3        3\n",
      "2342       2        2\n",
      "2343       3        3\n",
      "2344       0        0\n",
      "2345       3        3\n",
      "2346       2        2\n",
      "2347       3        3\n",
      "2348       2        2\n",
      "2349       1        1\n",
      "2350       1        1\n",
      "2351       0        0\n",
      "2352       2        2\n",
      "2353       2        2\n",
      "2354       0        0\n",
      "2355       0        0\n",
      "2356       1        1\n",
      "2357       1        1\n",
      "2358       2        2\n",
      "2359       2        2\n",
      "2360       1        1\n",
      "2361       0        1\n",
      "\n",
      "[2362 rows x 2 columns]\n",
      "name of each cluster base on the weight of TfIdf:\n",
      "cluster 0: edu.math (label 0 in excel)\n",
      "cluster 1: edu.sport (label 2 in excel)\n",
      "cluster 2: edu.engineering (label 1 in excel)\n",
      "cluster 3: edu.astronomy.geography (label 3 in excel)\n",
      "\n",
      "------ end of task 2 ------\n"
     ]
    }
   ],
   "source": [
    "#Assignment 5: Arrays and Text Clustering\n",
    "#Task 1: Data normalization using array operations and broadcasting\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "\n",
    "\n",
    "def minmax_norm(array):\n",
    "    \n",
    "    min_arr = [];\n",
    "    max_min_arr = [];\n",
    "    nomalized_arr = [];\n",
    "    min_arr = np.amin(array, axis=1)\n",
    "    max_arr = np.amax(array, axis=1)\n",
    "    max_min_arr = max_arr-min_arr;\n",
    "\n",
    "    print array\n",
    "    print min_arr\n",
    "    print max_min_arr\n",
    "    nomalized_arr = ((0.0+array.T-min_arr)/max_min_arr).T\n",
    "    \n",
    "    print \"------ task 1 ------\"\n",
    "    print \"input:\"\n",
    "    print array\n",
    "    print \"shape of the input:\"\n",
    "    print array.shape\n",
    "    print \"shape of the output:\"\n",
    "    print nomalized_arr.shape\n",
    "    print \"ouput:\"\n",
    "    print nomalized_arr\n",
    "    print \"------ end of task 1 ------\\n\"\n",
    "    \n",
    "# MAIN BLOCK\n",
    "if __name__ == \"__main__\": \n",
    "    array = np.random.randint(0,10,(4,3))\n",
    "    minmax_norm(array);\n",
    "\n",
    "#Task 2: Text Clustering\n",
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn import metrics\n",
    "\n",
    "    print \"------ task 2 ------\"\n",
    "    data=pickle.load(open(\"4NewsGroup.pkl\",\"r\"))\n",
    "\n",
    "    text,target=zip(*data)\n",
    "    text=list(text)\n",
    "    target=list(target)\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\", min_df=10, max_df=0.9) \n",
    "    dtm= tfidf_vect.fit_transform(text)\n",
    "\n",
    "    num_clusters = 4\n",
    "\n",
    "    clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=5)\n",
    "    clusters = clusterer.cluster(dtm.toarray(), assign_clusters=True)\n",
    "\n",
    "    order_centroids = np.array(clusterer.means()).argsort()[:, ::-1] \n",
    "    voc=tfidf_vect.vocabulary_\n",
    "    voc_lookup={tfidf_vect.vocabulary_[word]:word \\\n",
    "                for word in tfidf_vect.vocabulary_}\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        top_words=[voc_lookup[word_index] \\\n",
    "                   for word_index in order_centroids[i, :20]]\n",
    "        print(\"Cluster %d: %s \" % (i, \"; \".join(top_words)))\n",
    "\n",
    "\n",
    "    cluster_dict={0:0, 1:2, 2:1, 3:3}\n",
    "    predicted_clusters=[cluster_dict[i] for i in clusters]\n",
    " \n",
    "    #print target\n",
    "    print(metrics.classification_report(target, predicted_clusters))\n",
    "    \n",
    "    # get confusion matrix\n",
    "    df=pd.DataFrame(zip(target, predicted_clusters), columns=['actual','predict'])\n",
    "    df.groupby(['predict','actual']).size()\n",
    "    print df\n",
    "\n",
    "    # according to confusion matrix, we can get cluster_dict as follows:\n",
    "    # {0:0, 1:2, 2:1, 3:3}\n",
    "    # also we can get this conclusion by scanning and labeling(adding meaningful labels) the sample in the excel first, \n",
    "    # then mapping with the name of the clusters if they looks similar.\n",
    "    \n",
    "    ## Write down the name of each cluster as comments in your code.\n",
    "    # cluster 0: edu.math (label 0 in excel)\n",
    "    # cluster 1: edu.sport (label 2 in excel)\n",
    "    # cluster 2: edu.engineering (label 1 in excel)\n",
    "    # cluster 3: edu.astronomy.geography (label 3 in excel)\n",
    "\n",
    "    print \"name of each cluster base on the weight of TfIdf:\"\n",
    "    print \"cluster 0: edu.math (label 0 in excel)\\ncluster 1: edu.sport (label 2 in excel)\\ncluster 2: edu.engineering (label 1 in excel)\\ncluster 3: edu.astronomy.geography (label 3 in excel)\\n\"\n",
    "    print \"------ end of task 2 ------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
